{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d55901",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "604255bf51d44f09a9a3768c32edda85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/929 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gech\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\gech\\.cache\\huggingface\\hub\\models--cardiffnlp--twitter-roberta-base-sentiment-latest. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\gech\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58b49e9679734e33b399257b240fe6c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/501M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load a pre-trained sentiment model (e.g., for feedback on dynamic pricing)\n",
    "classifier = pipeline(\"sentiment-analysis\", model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
    "\n",
    "# Example: Analyze a pricing-related review\n",
    "review = \"The new price hike makes this product unaffordable â€“ terrible decision!\"\n",
    "result = classifier(review)\n",
    "print(result)  # Output: [{'label': 'NEGATIVE', 'score': 0.95}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f3806b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\gech\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'HuggingFaceEmbeddings' from 'transformers' (c:\\Users\\gech\\anaconda3\\Lib\\site-packages\\transformers\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     11\u001b[0m     pipeline, AutoTokenizer, AutoModelForCausalLM, \n\u001b[0;32m     12\u001b[0m     AutoModelForSequenceClassification, TrainingArguments, Trainer,\n\u001b[0;32m     13\u001b[0m     BitsAndBytesConfig, HuggingFaceEmbeddings\n\u001b[0;32m     14\u001b[0m )\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'HuggingFaceEmbeddings' from 'transformers' (c:\\Users\\gech\\anaconda3\\Lib\\site-packages\\transformers\\__init__.py)"
     ]
    }
   ],
   "source": [
    "# marketplace_pricing_explainer.py\n",
    "# Full Sample Project: Marketplace Pricing Explainer with Hugging Face Transformers\n",
    "# Author: Grok (based on job-aligned implementation)\n",
    "# Date: September 17, 2025\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import (\n",
    "    pipeline, AutoTokenizer, AutoModelForCausalLM, \n",
    "    AutoModelForSequenceClassification, TrainingArguments, Trainer,\n",
    "    BitsAndBytesConfig, HuggingFaceEmbeddings\n",
    ")\n",
    "from datasets import Dataset\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import xgboost as xgb\n",
    "from fastapi import FastAPI\n",
    "import uvicorn\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "# Step 1: Data Generation/ELT Pipeline (Simulate Web Scraping & Warehouse Integration)\n",
    "# Generate synthetic marketplace data: products, prices, demand, seasonality, inventory, competitors\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "data = {\n",
    "    'product_id': range(1, n_samples + 1),\n",
    "    'demand': np.random.uniform(50, 500, n_samples),  # Supply/demand proxy\n",
    "    'seasonality': np.random.choice(['Q1', 'Q2', 'Q3', 'Q4'], n_samples),  # Seasonal factor\n",
    "    'inventory': np.random.randint(10, 1000, n_samples),\n",
    "    'competitor_price': np.random.uniform(10, 200, n_samples),\n",
    "    'price': np.random.uniform(20, 250, n_samples),  # Target: Actual price\n",
    "    'elasticity': np.random.uniform(-2, -0.5, n_samples),  # Price elasticity (negative for normal goods)\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Feature Engineering (ELT: Transform for ML/BI)\n",
    "df['seasonal_factor'] = df['seasonality'].map({'Q1': 0.8, 'Q2': 1.0, 'Q3': 1.2, 'Q4': 1.5})\n",
    "df['demand_log'] = np.log(df['demand'] + 1)\n",
    "df['elasticity_abs'] = np.abs(df['elasticity'])  # For modeling\n",
    "\n",
    "# Simulate scraped competitor trends (in production: Use Scrapy/BeautifulSoup)\n",
    "df['trend'] = 'Increasing due to holidays' if np.mean(df['competitor_price']) > 100 else 'Stable'\n",
    "\n",
    "# Causal Documents for RAG (Knowledge Base: Pricing rules, explanations)\n",
    "knowledge_docs = [\n",
    "    \"High demand and low inventory cause price increases due to elasticity principles.\",\n",
    "    \"Seasonality in Q4 boosts prices by 20-30% for holiday demand.\",\n",
    "    \"Competitor pricing analysis: If competitors raise by 10%, match to maintain market share.\",\n",
    "    \"Causal inference: Supply shortages directly impact elasticity, leading to higher prices.\",\n",
    "    \"Forecasting tip: Use XGBoost for predicting optimal price based on demand and seasonality.\"\n",
    "]\n",
    "print(\"Data loaded:\", df.shape)\n",
    "print(\"\\nSample Data:\\n\", df.head())\n",
    "\n",
    "# Step 2: Classical ML - Dynamic Pricing Engine (XGBoost for Forecasting & Elasticity)\n",
    "# Features: demand_log, seasonal_factor, inventory, competitor_price, elasticity_abs\n",
    "X = df[['demand_log', 'seasonal_factor', 'inventory', 'competitor_price', 'elasticity_abs']]\n",
    "y = df['price']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, learning_rate=0.1)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict & Evaluate (Elasticity-based: Adjust price by elasticity)\n",
    "y_pred = model.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(f\"\\nClassical ML: XGBoost MAE = {mae:.2f}\")\n",
    "\n",
    "# Example Prediction (Simulate real-time query)\n",
    "sample_input = np.array([[np.log(300+1), 1.5, 50, 150, 1.2]])  # High demand, Q4, low inventory, high competitor\n",
    "predicted_price = model.predict(sample_input)[0]\n",
    "print(f\"Predicted Optimal Price: ${predicted_price:.2f}\")\n",
    "\n",
    "# Causal Inference Teaser (Simple: Compute elasticity impact)\n",
    "elasticity_impact = predicted_price * df['elasticity_abs'].mean()\n",
    "print(f\"Estimated Elasticity Adjustment: -${elasticity_impact:.2f}\")\n",
    "\n",
    "# Step 3: Hugging Face Transformers - Embeddings & RAG Setup\n",
    "# Use SentenceTransformer (Hugging Face) for embeddings\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')  # Lightweight for BI\n",
    "doc_embeddings = embedder.encode(knowledge_docs)\n",
    "\n",
    "# FAISS Index for Retrieval\n",
    "dimension = len(doc_embeddings[0])\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(np.array(doc_embeddings))\n",
    "\n",
    "def retrieve_docs(query, k=2):\n",
    "    query_emb = embedder.encode([query])\n",
    "    distances, indices = index.search(query_emb, k)\n",
    "    return [knowledge_docs[i] for i in indices[0]]\n",
    "\n",
    "# Step 4: LLM Fine-Tuning for Domain-Specific Explanations\n",
    "# Fine-tune DistilGPT-2 on synthetic pricing Q&A pairs (for \"why\" insights)\n",
    "# Dataset: 100 samples of (query, explanation)\n",
    "synthetic_data = []\n",
    "for i in range(100):\n",
    "    query = f\"Why is the price ${np.random.uniform(100,200):.2f} for product with demand {np.random.uniform(100,400):.0f}?\"\n",
    "    explanation = f\"Price is high due to {'high demand and low elasticity' if np.random.rand()>0.5 else 'seasonal factors and competitor pricing'}.\"\n",
    "    synthetic_data.append({'text': f\"Q: {query} A: {explanation}\"})\n",
    "\n",
    "train_dataset = Dataset.from_list(synthetic_data)\n",
    "\n",
    "# Tokenizer & Model Setup\n",
    "model_name = \"distilgpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=128)\n",
    "\n",
    "tokenized_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Training Args (Efficient: Use PEFT/LoRA for full-scale)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./fine_tuned_pricing_explainer\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    save_steps=50,\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"no\",  # Simple for demo\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Fine-Tune (Run this; ~2-3 mins on CPU)\n",
    "trainer.train()\n",
    "trainer.save_model(\"./fine_tuned_pricing_explainer\")\n",
    "print(\"\\nFine-Tuning Complete: Model saved to ./fine_tuned_pricing_explainer\")\n",
    "\n",
    "# Load Fine-Tuned Model for Inference\n",
    "fine_tuned_model = AutoModelForCausalLM.from_pretrained(\"./fine_tuned_pricing_explainer\")\n",
    "fine_tuned_tokenizer = AutoTokenizer.from_pretrained(\"./fine_tuned_pricing_explainer\")\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=fine_tuned_model, tokenizer=fine_tuned_tokenizer, max_length=150)\n",
    "\n",
    "# Step 5: RAG-Enhanced Generation (Integrate Classical ML + RAG + LLM)\n",
    "def generate_explanation(query, predicted_price):\n",
    "    # Retrieve relevant docs\n",
    "    retrieved = retrieve_docs(query)\n",
    "    context = \" \".join(retrieved)\n",
    "    \n",
    "    # Prompt with ML output + RAG context\n",
    "    prompt = f\"Q: {query} Predicted Price: ${predicted_price:.2f}. Context: {context} A: Explain causally why this price.\"\n",
    "    \n",
    "    # Generate with fine-tuned LLM\n",
    "    output = generator(prompt, num_return_sequences=1, temperature=0.7)[0]['generated_text']\n",
    "    return output.split(\"A: \")[-1].strip()  # Extract explanation\n",
    "\n",
    "# Example Usage (BI Insight)\n",
    "query = \"Why should we set this price given high Q4 demand and competitors?\"\n",
    "explanation = generate_explanation(query, predicted_price)\n",
    "print(f\"\\nGenAI Explanation: {explanation}\")\n",
    "\n",
    "# Step 6: Visualization Layer (Teaser for Apache Superset)\n",
    "# Plot price forecasts vs. actual\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(y_test, y_pred)\n",
    "plt.xlabel('Actual Price')\n",
    "plt.ylabel('Predicted Price')\n",
    "plt.title('XGBoost Forecasting')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "df.groupby('seasonality')['price'].mean().plot(kind='bar')\n",
    "plt.title('Seasonal Pricing Trends')\n",
    "plt.ylabel('Avg Price')\n",
    "plt.tight_layout()\n",
    "plt.savefig('pricing_viz.png')\n",
    "plt.show()\n",
    "print(\"\\nVisualization saved: pricing_viz.png (Adapt to Superset via SQL export)\")\n",
    "\n",
    "# Step 7: API Deployment (MLOps: FastAPI for Production-Grade)\n",
    "app = FastAPI(title=\"Marketplace Pricing Explainer API\")\n",
    "\n",
    "@app.post(\"/predict_and_explain\")\n",
    "def predict_explain(input_data: dict):\n",
    "    # Parse input (e.g., from ops team)\n",
    "    features = np.array([[input_data['demand_log'], input_data['seasonal_factor'], \n",
    "                          input_data['inventory'], input_data['competitor_price'], input_data['elasticity_abs']]])\n",
    "    pred_price = model.predict(features)[0]\n",
    "    query = input_data.get('query', 'Why this price?')\n",
    "    expl = generate_explanation(query, pred_price)\n",
    "    return {\"predicted_price\": pred_price, \"explanation\": expl}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run API: uvicorn marketplace_pricing_explainer:app --reload\n",
    "    print(\"\\nAPI Ready: Run 'uvicorn marketplace_pricing_explainer:app --reload' for deployment.\")\n",
    "    # For Docker: Add Dockerfile with FROM python:3.12, COPY ., RUN pip install -r requirements.txt, CMD [\"uvicorn...\"]\n",
    "\n",
    "# Example API Call (Simulate)\n",
    "sample_api_input = {\n",
    "    \"demand_log\": np.log(300+1),\n",
    "    \"seasonal_factor\": 1.5,\n",
    "    \"inventory\": 50,\n",
    "    \"competitor_price\": 150,\n",
    "    \"elasticity_abs\": 1.2,\n",
    "    \"query\": \"Why raise price in holidays?\"\n",
    "}\n",
    "# In practice: Use requests.post(\"http://localhost:8000/predict_and_explain\", json=sample_api_input)\n",
    "print(\"\\nSample API Response:\", json.dumps({\"predicted_price\": predicted_price, \"explanation\": explanation}, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcadaa0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
